{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2-4_DsLdPo_z"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded: (30240, 4)\n",
      "   Message ID                       Subject  \\\n",
      "0           1           vastar resource inc   \n",
      "1           2  calpine daily gas nomination   \n",
      "2           3                      re issue   \n",
      "3           4              meter allocation   \n",
      "4           5        mcmullen gas for 11 99   \n",
      "\n",
      "                                             Message  Spam/Ham  \n",
      "0  gary production from the high island larger bl...         0  \n",
      "1                   calpine daily gas nomination doc         0  \n",
      "2  see note below already done stella stella this...         0  \n",
      "3  kimberly vaughn lauri i have put this on stran...         0  \n",
      "4  jackie since the inlet to river plant is shut ...         0  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv('dataset_2.csv')\n",
    "print(f\"Data loaded: {data.shape}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8qlOdvaBPvNk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (30240, 5)\n",
      "Columns: ['Message ID', 'Subject', 'Message', 'Spam/Ham', 'cleaned_text']\n",
      "Texts before filtering: 30240\n",
      "Texts after filtering: 30239\n",
      "Label distribution:\n",
      "label\n",
      "1    15159\n",
      "0    15080\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/z9wl147d2zv2nlrt9vg113wr0000gn/T/ipykernel_65201/2223504506.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['label'] = data[label_col].astype(int)\n",
      "/var/folders/nc/z9wl147d2zv2nlrt9vg113wr0000gn/T/ipykernel_65201/2223504506.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text_length'] = data['cleaned_text'].str.len()\n",
      "/var/folders/nc/z9wl147d2zv2nlrt9vg113wr0000gn/T/ipykernel_65201/2223504506.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['word_count'] = data['cleaned_text'].apply(lambda x: len(str(x).split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4852 duplicates\n",
      "Final shape after outlier removal: (25349, 9)\n",
      "\n",
      "=== BALANCING DATASET ===\n",
      "Spam samples: 12341\n",
      "Ham samples: 13008\n",
      "Balanced dataset shape: (24682, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nc/z9wl147d2zv2nlrt9vg113wr0000gn/T/ipykernel_65201/2223504506.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['uppercase_ratio'] = data[text_col].apply(\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Preprocess and clean\n",
    "def preprocess_and_clean(data):\n",
    "    \"\"\"Preprocess and clean spam data\"\"\"\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    text_col = 'Message'\n",
    "    label_col = 'Spam/Ham'  # 0 = ham, 1 = spam\n",
    "\n",
    "    print(f\"Original shape: {data.shape}\")\n",
    "    print(f\"Columns: {data.columns.tolist()}\")\n",
    "\n",
    "    # Clean text\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        text = re.sub(r'\\\\n', ' ', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?@:/-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    # Create cleaned_text\n",
    "    data['cleaned_text'] = data[text_col].apply(clean_text)\n",
    "\n",
    "    print(f\"Texts before filtering: {len(data)}\")\n",
    "\n",
    "    # Keep only non-empty messages\n",
    "    data = data[data['cleaned_text'].str.len() > 1]\n",
    "    print(f\"Texts after filtering: {len(data)}\")\n",
    "\n",
    "    # Fix label mapping for numeric dataset\n",
    "    data['label'] = data[label_col].astype(int)\n",
    "\n",
    "    print(f\"Label distribution:\\n{data['label'].value_counts()}\")\n",
    "\n",
    "    # Feature extraction\n",
    "    data['text_length'] = data['cleaned_text'].str.len()\n",
    "    data['word_count'] = data['cleaned_text'].apply(lambda x: len(str(x).split()))\n",
    "    data['uppercase_ratio'] = data[text_col].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)\n",
    "    )\n",
    "\n",
    "    # Remove duplicates\n",
    "    initial = len(data)\n",
    "    data = data.drop_duplicates(subset=['cleaned_text'])\n",
    "    print(f\"Removed {initial - len(data)} duplicates\")\n",
    "\n",
    "    # Handle outliers gently\n",
    "    for col in ['text_length', 'word_count']:\n",
    "        if data[col].nunique() > 1:\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 3 * IQR\n",
    "            upper = Q3 + 3 * IQR\n",
    "            data = data[(data[col] >= lower) & (data[col] <= upper)]\n",
    "\n",
    "    print(f\"Final shape after outlier removal: {data.shape}\")\n",
    "\n",
    "    # Balance dataset\n",
    "    print(\"\\n=== BALANCING DATASET ===\")\n",
    "    spam_data = data[data['label'] == 1]\n",
    "    ham_data = data[data['label'] == 0]\n",
    "\n",
    "    print(f\"Spam samples: {len(spam_data)}\")\n",
    "    print(f\"Ham samples: {len(ham_data)}\")\n",
    "\n",
    "    if len(spam_data) > 0 and len(ham_data) > 0:\n",
    "        min_samples = min(len(spam_data), len(ham_data))\n",
    "        spam_balanced = spam_data.sample(n=min_samples, random_state=42)\n",
    "        ham_balanced = ham_data.sample(n=min_samples, random_state=42)\n",
    "        data = pd.concat([spam_balanced, ham_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Balanced dataset shape: {data.shape}\")\n",
    "    else:\n",
    "        print(\"Skipping balancing (one label class missing).\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "cleaned_data = preprocess_and_clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "k3rXmeIhP2RB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizing features...\n",
      "Sample cleaned texts:\n",
      "0    attached is the open season posting for tw s p...\n",
      "1    hey better than all other spam filter only del...\n",
      "2    ford salute the military this offer is extende...\n",
      "3    won t take our name aep mirant powerex limited...\n",
      "4    increase your cum volume and orgasm length mai...\n",
      "Name: cleaned_text, dtype: object\n",
      "\n",
      "Non-empty texts: 24682\n",
      "TF-IDF vocabulary size: 10000\n",
      "Features shape: (24682, 10003)\n",
      "Combined feature shape: (24682, 10003)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "print(\"\\nVectorizing features...\")\n",
    "\n",
    "# Check cleaned text\n",
    "print(f\"Sample cleaned texts:\")\n",
    "print(cleaned_data['cleaned_text'].head())\n",
    "print(f\"\\nNon-empty texts: {(cleaned_data['cleaned_text'].str.len() > 0).sum()}\")\n",
    "\n",
    "# TF-IDF with more lenient settings\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "try:\n",
    "    X_tfidf = tfidf.fit_transform(cleaned_data['cleaned_text'])\n",
    "    print(f\"TF-IDF vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Trying without stop words...\")\n",
    "    # Retry without stop words if vocabulary is empty\n",
    "    tfidf = TfidfVectorizer( \n",
    "        max_features=3000, \n",
    "        ngram_range=(1,2),\n",
    "        min_df=1\n",
    "    )\n",
    "    X_tfidf = tfidf.fit_transform(cleaned_data['cleaned_text'])\n",
    "\n",
    "# Numeric features - DON'T scale for Naive Bayes compatibility\n",
    "X_numeric = cleaned_data[['text_length', 'word_count', 'uppercase_ratio']].values\n",
    "\n",
    "# Combine\n",
    "X = sparse.hstack([X_tfidf, X_numeric])\n",
    "y = cleaned_data['label'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "\n",
    "# Save preprocessing tools\n",
    "with open('tfidf_vectoriser.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Save a scaler for the API (even though we don't use it for training)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cleaned_data[['text_length', 'word_count', 'uppercase_ratio']])\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "numeric_features = cleaned_data[['text_length', 'word_count', 'uppercase_ratio']].values\n",
    "X_combined = hstack([X_tfidf, sparse.csr_matrix(numeric_features)])\n",
    "print(\"Combined feature shape:\", X_combined.shape)\n",
    "\n",
    "# Use combined features going forward\n",
    "X = X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "8s8w6jqPQD_K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING MODELS\n",
      "==================================================\n",
      "Train: (19745, 10003), Test: (4937, 10003)\n",
      "Train spam: 9873, ham: 9872\n",
      "Test spam: 2468, ham: 2469\n",
      "\n",
      "Running Grid Search for Logistic Regression...\n",
      "Best Logistic Regression params: {'C': 5, 'solver': 'liblinear'}\n",
      "\n",
      "=== Naive Bayes ===\n",
      "Accuracy : 0.9682\n",
      "Precision: 0.9857\n",
      "Recall   : 0.9502\n",
      "F1-score : 0.9676\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy : 0.9526\n",
      "Precision: 0.9196\n",
      "Recall   : 0.9919\n",
      "F1-score : 0.9544\n",
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy : 0.9844\n",
      "Precision: 0.9799\n",
      "Recall   : 0.9891\n",
      "F1-score : 0.9845\n",
      "\n",
      "Building ensemble model...\n",
      "\n",
      "Ensemble Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      2469\n",
      "           1       0.98      0.99      0.98      2468\n",
      "\n",
      "    accuracy                           0.98      4937\n",
      "   macro avg       0.98      0.98      0.98      4937\n",
      "weighted avg       0.98      0.98      0.98      4937\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Model                Accuracy     Precision    Recall       F1-Score    \n",
      "----------------------------------------------------------------------\n",
      "Naive Bayes          0.9682       0.9857       0.9502       0.9676      \n",
      "Random Forest        0.9526       0.9196       0.9919       0.9544      \n",
      "Logistic Regression  0.9844       0.9799       0.9891       0.9845      \n",
      "\n",
      "BEST MODEL: Logistic Regression (F1-Score: 0.9845)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2419   50]\n",
      " [  27 2441]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.98      0.98      2469\n",
      "        Spam       0.98      0.99      0.98      2468\n",
      "\n",
      "    accuracy                           0.98      4937\n",
      "   macro avg       0.98      0.98      0.98      4937\n",
      "weighted avg       0.98      0.98      0.98      4937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and train ML models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train spam: {sum(y_train == 1)}, ham: {sum(y_train == 0)}\")\n",
    "print(f\"Test spam: {sum(y_test == 1)}, ham: {sum(y_test == 0)}\")\n",
    "\n",
    "print(\"\\nRunning Grid Search for Logistic Regression...\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 5, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Logistic Regression params:\", grid.best_params_)\n",
    "best_lr = grid.best_estimator_\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Handle imbalanced data\n",
    "        max_depth=20\n",
    "    ),\n",
    "    'Logistic Regression': best_lr,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nBuilding ensemble model...\")\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', best_lr),\n",
    "        ('rf', RandomForestClassifier(n_estimators=300, max_depth=30, random_state=42)),\n",
    "        ('nb', MultinomialNB())\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble.fit(X_train, y_train)\n",
    "ensemble_pred = ensemble.predict(X_test)\n",
    "\n",
    "print(\"\\nEnsemble Performance:\")\n",
    "print(classification_report(y_test, ensemble_pred))\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\"*70)\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<20} {res['acc']:<12.4f} {res['prec']:<12.4f} {res['rec']:<12.4f} {res['f1']:<12.4f}\")\n",
    "\n",
    "# Best model\n",
    "best_name = max(results, key=lambda x: results[x]['f1'])\n",
    "best_model = results[best_name]['model']\n",
    "print(f\"\\nBEST MODEL: {best_name} (F1-Score: {results[best_name]['f1']:.4f})\")\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL SANITY CHECK ===\n",
      "✓ Text: Hi, let's meet for coffee tomorrow at 3pm...\n",
      "   Expected: ham, Got: ham (confidence: 0.58)\n",
      "\n",
      "✓ Text: Meeting reminder: Project review at 2pm...\n",
      "   Expected: ham, Got: ham (confidence: 0.94)\n",
      "\n",
      "✗ Text: Your package has been delivered successfully...\n",
      "   Expected: ham, Got: spam (confidence: 0.79)\n",
      "\n",
      "✓ Text: WINNER! Claim your $1000000 prize NOW!!!...\n",
      "   Expected: spam, Got: spam (confidence: 0.92)\n",
      "\n",
      "✓ Text: Congratulations! You won the lottery! Send bank de...\n",
      "   Expected: spam, Got: spam (confidence: 0.87)\n",
      "\n",
      "✓ Text: URGENT: Verify your account or it will be closed...\n",
      "   Expected: spam, Got: spam (confidence: 0.83)\n",
      "\n",
      "Test Accuracy: 5/6 (83.3%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test the model\n",
    "def test_model_predictions():\n",
    "    \"\"\"Test model on sample emails\"\"\"\n",
    "    print(\"\\n=== MODEL SANITY CHECK ===\")\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"Hi, let's meet for coffee tomorrow at 3pm\", \"ham\"),\n",
    "        (\"Meeting reminder: Project review at 2pm\", \"ham\"),\n",
    "        (\"Your package has been delivered successfully\", \"ham\"),\n",
    "        (\"WINNER! Claim your $1000000 prize NOW!!!\", \"spam\"),\n",
    "        (\"Congratulations! You won the lottery! Send bank details\", \"spam\"),\n",
    "        (\"URGENT: Verify your account or it will be closed\", \"spam\"),\n",
    "    ]\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(test_cases)\n",
    "    \n",
    "    for text, expected in test_cases:\n",
    "        # Clean and process the text\n",
    "        cleaned = text.lower()\n",
    "        cleaned = re.sub(r'http\\S+|www\\S+', '', cleaned)\n",
    "        cleaned = re.sub(r'\\\\n', ' ', cleaned)\n",
    "        cleaned = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', cleaned)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "        \n",
    "        # Extract features\n",
    "        X_tfidf_test = tfidf.transform([cleaned])\n",
    "        text_length = len(cleaned)\n",
    "        word_count = len(cleaned.split())\n",
    "        uppercase_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "        X_numeric_test = np.array([[text_length, word_count, uppercase_ratio]])\n",
    "        X_test = sparse.hstack([X_tfidf_test, X_numeric_test])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = best_model.predict(X_test)[0]\n",
    "        probability = best_model.predict_proba(X_test)[0]\n",
    "        pred_label = 'spam' if prediction == 1 else 'ham'\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = pred_label == expected\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        \n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{status} Text: {text[:50]}...\")\n",
    "        print(f\"   Expected: {expected}, Got: {pred_label} (confidence: {probability[prediction]:.2f})\")\n",
    "        print()\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"Test Accuracy: {correct}/{total} ({accuracy:.1f}%)\")\n",
    "    \n",
    "    if accuracy < 50:\n",
    "        print(\"\\nWARNING: Model is performing poorly! Consider:\")\n",
    "        print(\"   1. Balancing your dataset\")\n",
    "        print(\"   2. Adding more training data\")\n",
    "        print(\"   3. Adjusting model parameters\")\n",
    "\n",
    "# Run the test\n",
    "test_model_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model saved as 'best_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(\"\\nBest model saved as 'best_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNeKMHNqngYjx6ETUlf2bxs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
